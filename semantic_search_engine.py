# -*- coding: utf-8 -*-
"""Copy of BANA 275 Semantic Search Engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1em_S3Vf5FYz6d1XhLEAP34le8UAFF1HJ
"""

#!pip -q install sentence-transformers pypdf scikit-learn tqdm



import os, re
import numpy as np
from tqdm import tqdm
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

PDF_DIR = "files"

MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
MAX_CHARS = 300
OVERLAP = 75
TOP_K = 10
BATCH_SIZE = 64

def clean_text(s: str) -> str:
    s = s.replace("-\n", "")
    s = s.replace("\n", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def extract_pdf_chunks(pdf_path: str, max_chars=900, overlap=120):
    reader = PdfReader(pdf_path)
    pdf_file = os.path.basename(pdf_path)
    chunks = []

    for i, page in enumerate(reader.pages):
        raw = page.extract_text() or ""
        text = clean_text(raw)
        if not text.strip():
            continue

        start = 0
        page_num = i + 1
        while start < len(text):
            end = min(start + max_chars, len(text))
            chunk_text = text[start:end].strip()
            if chunk_text:
                chunk_id = f"{pdf_file}::p{page_num}::c{len(chunks)}"
                chunks.append({
                    "chunk_id": chunk_id,
                    "pdf_file": pdf_file,
                    "page": page_num,
                    "text": chunk_text
                })
            if end == len(text):
                break
            start = max(0, end - overlap)

    return chunks

def build_index(pdf_dir: str):
    pdf_files = sorted([
        os.path.join(pdf_dir, f)
        for f in os.listdir(pdf_dir)
        if f.lower().endswith(".pdf")
    ])

    if not pdf_files:
        raise FileNotFoundError(f"No PDF files found in: {pdf_dir}")

    print("Found PDFs:")
    for f in pdf_files:
        print(" -", os.path.basename(f))

    # 1) chunks
    all_chunks = []
    for pdf in pdf_files:
        all_chunks.extend(extract_pdf_chunks(pdf, MAX_CHARS, OVERLAP))

    if len(all_chunks) == 0:
        raise ValueError("No text extracted. If these are scanned PDFs, you may need OCR.")

    print(f"\nTotal chunks: {len(all_chunks)}")

    # 2) embeddings
    model = SentenceTransformer(MODEL_NAME)
    texts = [c["text"] for c in all_chunks]

    embs = []
    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="Embedding chunks"):
        batch = texts[i:i+BATCH_SIZE]
        emb = model.encode(batch, normalize_embeddings=True)  # normalize -> cosine = dot
        embs.append(emb)

    embeddings = np.vstack(embs).astype(np.float32)
    return all_chunks, embeddings, model

def search(query: str, chunks, embeddings, model, top_k=5):
    q_emb = model.encode([query], normalize_embeddings=True).astype(np.float32)
    scores = cosine_similarity(q_emb, embeddings)[0]
    idx = np.argsort(-scores)[:top_k]

    results = []
    for rank, j in enumerate(idx, start=1):
        c = chunks[j]
        results.append({
            "rank": rank,
            "score": float(scores[j]),
            "pdf_file": c["pdf_file"],
            "page": c["page"],
            "chunk_id": c["chunk_id"],
            "preview": c["text"][:300] + ("..." if len(c["text"]) > 300 else "")
        })
    return results

if __name__ == "__main__":
    all_chunks, embeddings, model = build_index(pdf_dir=PDF_DIR)

    chunks, embeddings, model = build_index(PDF_DIR)

    print("\nIndex ready. Try a query like:")
    print('results = search("your query", chunks, embeddings, model, top_k=5)')

    results = search("Risks of AI", chunks, embeddings, model, top_k=5)
    for r in results:
        print(f"\n#{r['rank']}  score={r['score']:.4f}")
        print(f"Source: {r['pdf_file']}  page={r['page']}  id={r['chunk_id']}")
        print(r["preview"])

    results = search("What is machine learning?", chunks, embeddings, model, top_k=5)
    for r in results:
        print(f"\n#{r['rank']}  score={r['score']:.4f}")
        print(f"Source: {r['pdf_file']}  page={r['page']}  id={r['chunk_id']}")
        print(r["preview"])